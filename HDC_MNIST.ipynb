{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPFib3zUqfIa",
        "outputId": "80cc8298-d4c8-4861-c2f1-6e1324e6a27a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.72,\n",
              " 'Cosine Similarities (example)': [[0.05469023514409879,\n",
              "   0.38723056907918124,\n",
              "   0.13230486553120455,\n",
              "   0.13452673573850304]],\n",
              " 'Latent Component Explained Variance Ratio': [0.16977878709720945,\n",
              "  0.10842187016820352,\n",
              "  0.05788565452195865,\n",
              "  0.047540718585688285,\n",
              "  0.043384314224870636,\n",
              "  0.04006247995360765,\n",
              "  0.036778212717841686,\n",
              "  0.02987207864306066,\n",
              "  0.025010354804586472,\n",
              "  0.022676241564176122]}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Step 1: Generate synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=500, n_features=20, n_informative=15, n_classes=3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Hyperdimensional Computing (HDC) Encoding\n",
        "def encode_to_hypervectors(data, dim=1000):\n",
        "    np.random.seed(42)\n",
        "    projection_matrix = np.random.randn(data.shape[1], dim)\n",
        "    hypervectors = np.dot(data, projection_matrix)\n",
        "    return np.sign(hypervectors)  # Binarize to create hypervectors\n",
        "\n",
        "# Encode data to hypervectors\n",
        "hdc_vectors = encode_to_hypervectors(X)\n",
        "\n",
        "# Step 3: Perform PCA (Latent Component Analysis)\n",
        "pca = PCA(n_components=10)\n",
        "latent_components = pca.fit_transform(hdc_vectors)\n",
        "\n",
        "# Step 4: Combine HDC and LCM features\n",
        "combined_features = np.hstack([hdc_vectors, latent_components])\n",
        "\n",
        "# Normalize combined features for downstream use\n",
        "normalized_features = normalize(combined_features)\n",
        "\n",
        "# Step 5: Train/Test split for classification\n",
        "split = int(0.8 * len(normalized_features))\n",
        "X_train, X_test = normalized_features[:split], normalized_features[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Step 6: Classify with SVM\n",
        "classifier = SVC(kernel=\"linear\", random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cos_sim = cosine_similarity(X_test[:1], X_test[1:5])  # Example of similarity measurement\n",
        "\n",
        "# Output results\n",
        "{\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Cosine Similarities (example)\": cos_sim.tolist(),\n",
        "    \"Latent Component Explained Variance Ratio\": pca.explained_variance_ratio_.tolist(),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Step 1: Optimize HDC dimensionality and PCA components\n",
        "hdc_dimensions = [500, 1000, 2000]\n",
        "pca_components = [5, 10, 15]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "for hdc_dim in hdc_dimensions:\n",
        "    # Re-encode data to hypervectors with new dimensionality\n",
        "    hdc_vectors = encode_to_hypervectors(X, dim=hdc_dim)\n",
        "\n",
        "    for pca_comp in pca_components:\n",
        "        # Perform PCA with a different number of components\n",
        "        pca = PCA(n_components=pca_comp)\n",
        "        latent_components = pca.fit_transform(hdc_vectors)\n",
        "\n",
        "        # Combine HDC and LCM features\n",
        "        combined_features = np.hstack([hdc_vectors, latent_components])\n",
        "        normalized_features = normalize(combined_features)\n",
        "\n",
        "        # Train/Test split for classification\n",
        "        X_train, X_test = normalized_features[:split], normalized_features[split:]\n",
        "        y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "        # Train SVM classifier with grid search for hyperparameters\n",
        "        param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "        classifier = GridSearchCV(SVC(random_state=42), param_grid, cv=3)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "\n",
        "        # Evaluate performance\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Store the best result\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params = (hdc_dim, pca_comp, classifier.best_params_)\n",
        "            best_model = classifier\n",
        "\n",
        "# Step 2: Output optimized results\n",
        "{\n",
        "    \"Best Accuracy\": best_accuracy,\n",
        "    \"Optimal HDC Dimension\": best_params[0],\n",
        "    \"Optimal PCA Components\": best_params[1],\n",
        "    \"Optimal SVM Parameters\": best_params[2],\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpgQczkYsHtV",
        "outputId": "8b99d61d-6978-4984-e90f-5084ce14117d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Best Accuracy': 0.88,\n",
              " 'Optimal HDC Dimension': 500,\n",
              " 'Optimal PCA Components': 5,\n",
              " 'Optimal SVM Parameters': {'C': 10, 'kernel': 'rbf'}}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Step 1: Load Fashion MNIST dataset (10% subset)\n",
        "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
        "X_fashion, y_fashion = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
        "\n",
        "# Sample 10% of the data for faster processing\n",
        "sample_size = int(0.1 * len(X_fashion))\n",
        "X_fashion, _, y_fashion, _ = train_test_split(X_fashion, y_fashion, train_size=sample_size, stratify=y_fashion, random_state=42)\n",
        "\n",
        "# Step 2: Preprocess Fashion MNIST data\n",
        "scaler = StandardScaler()\n",
        "X_fashion_scaled = scaler.fit_transform(X_fashion)\n",
        "\n",
        "# Step 3: HDC Encoding and PCA Optimization\n",
        "optimal_hdc_dim = 500  # From previous optimization\n",
        "optimal_pca_comp = 5   # From previous optimization\n",
        "\n",
        "# Encode to hypervectors\n",
        "hdc_vectors_fashion = encode_to_hypervectors(X_fashion_scaled, dim=optimal_hdc_dim)\n",
        "\n",
        "# Perform PCA\n",
        "pca_fashion = PCA(n_components=optimal_pca_comp)\n",
        "latent_components_fashion = pca_fashion.fit_transform(hdc_vectors_fashion)\n",
        "\n",
        "# Combine HDC and LCM features\n",
        "combined_features_fashion = np.hstack([hdc_vectors_fashion, latent_components_fashion])\n",
        "normalized_features_fashion = normalize(combined_features_fashion)\n",
        "\n",
        "# Step 4: Train/Test split\n",
        "X_train_fashion, X_test_fashion, y_train_fashion, y_test_fashion = train_test_split(\n",
        "    normalized_features_fashion, y_fashion, test_size=0.2, random_state=42, stratify=y_fashion\n",
        ")\n",
        "\n",
        "# Step 5: Train and Evaluate SVM with optimal parameters\n",
        "svm_classifier_fashion = SVC(C=10, kernel='rbf', random_state=42)\n",
        "svm_classifier_fashion.fit(X_train_fashion, y_train_fashion)\n",
        "y_pred_fashion = svm_classifier_fashion.predict(X_test_fashion)\n",
        "\n",
        "# Step 6: Evaluate performance on Fashion MNIST\n",
        "fashion_accuracy = accuracy_score(y_test_fashion, y_pred_fashion)\n",
        "fashion_cos_sim = cosine_similarity(X_test_fashion[:1], X_test_fashion[1:5])  # Example similarity\n",
        "\n",
        "# Output results\n",
        "{\n",
        "    \"Fashion MNIST Accuracy\": fashion_accuracy,\n",
        "    \"Cosine Similarities (example)\": fashion_cos_sim.tolist(),\n",
        "    \"Latent Component Explained Variance Ratio\": pca_fashion.explained_variance_ratio_.tolist(),\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqPewpjFs4OL",
        "outputId": "ddda517a-69db-49d5-c6cb-f06f9ebc5d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Fashion MNIST Accuracy': 0.8328571428571429,\n",
              " 'Cosine Similarities (example)': [[-0.4022284590230415,\n",
              "   -0.31704810072676737,\n",
              "   0.13938251078071415,\n",
              "   -0.15629955804816287]],\n",
              " 'Latent Component Explained Variance Ratio': [0.1697821374874963,\n",
              "  0.10693141187758717,\n",
              "  0.050897886418453504,\n",
              "  0.02984687155923866,\n",
              "  0.025504891240072103]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload Fashion MNIST data (10% subset) and preprocess\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Reload the dataset\n",
        "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
        "X_fashion, y_fashion = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
        "\n",
        "# Sample 10% of the data for faster processing\n",
        "sample_size = int(0.1 * len(X_fashion))\n",
        "X_fashion, _, y_fashion, _ = train_test_split(X_fashion, y_fashion, train_size=sample_size, stratify=y_fashion, random_state=42)\n",
        "\n",
        "# Preprocess data\n",
        "scaler = StandardScaler()\n",
        "X_fashion_scaled = scaler.fit_transform(X_fashion)\n",
        "\n",
        "# Pure HDC Encoding and Evaluation\n",
        "# Step 1: Encode to hypervectors using optimal dimension (500)\n",
        "pure_hdc_vectors_fashion = encode_to_hypervectors(X_fashion_scaled, dim=optimal_hdc_dim)\n",
        "\n",
        "# Step 2: Normalize HDC features\n",
        "pure_hdc_normalized = normalize(pure_hdc_vectors_fashion)\n",
        "\n",
        "# Step 3: Train/Test split for pure HDC model\n",
        "X_train_pure_hdc, X_test_pure_hdc, y_train_pure_hdc, y_test_pure_hdc = train_test_split(\n",
        "    pure_hdc_normalized, y_fashion, test_size=0.2, random_state=42, stratify=y_fashion\n",
        ")\n",
        "\n",
        "# Step 4: Train and Evaluate SVM on pure HDC features\n",
        "svm_classifier_pure_hdc = SVC(C=10, kernel='rbf', random_state=42)\n",
        "svm_classifier_pure_hdc.fit(X_train_pure_hdc, y_train_pure_hdc)\n",
        "y_pred_pure_hdc = svm_classifier_pure_hdc.predict(X_test_pure_hdc)\n",
        "\n",
        "# Step 5: Evaluate performance of pure HDC model\n",
        "pure_hdc_accuracy = accuracy_score(y_test_pure_hdc, y_pred_pure_hdc)\n",
        "pure_hdc_cos_sim = cosine_similarity(X_test_pure_hdc[:1], X_test_pure_hdc[1:5])  # Example similarity\n",
        "\n",
        "# Output results for pure HDC\n",
        "{\n",
        "    \"Pure HDC Accuracy\": pure_hdc_accuracy,\n",
        "    \"Cosine Similarities (example)\": pure_hdc_cos_sim.tolist(),\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWrdmL4ntekY",
        "outputId": "7965ca41-a94f-48e7-e2d7-208193d28835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Pure HDC Accuracy': 0.8342857142857143,\n",
              " 'Cosine Similarities (example)': [[-0.2720000000000001,\n",
              "   -0.192,\n",
              "   0.036,\n",
              "   -0.072]]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define the Schema\n",
        "DIMENSIONS = 100\n",
        "SCHEMA = {\n",
        "    \"Attributes\": slice(0, 10),       # Dimensions 0-9\n",
        "    \"Actions\": slice(10, 20),        # Dimensions 10-19\n",
        "    \"Context\": slice(20, 30),        # Dimensions 20-29\n",
        "    \"Relationships\": slice(30, 40),  # Dimensions 30-39\n",
        "    \"Entity Type\": slice(40, 50),    # Dimensions 40-49\n",
        "    \"Reserved\": slice(50, DIMENSIONS)  # Dimensions 50-99\n",
        "}\n",
        "\n",
        "# Step 2: Helper Functions\n",
        "def generate_random_hypervector(dim):\n",
        "    \"\"\"Generate a random sparse hypervector with 50% sparsity.\"\"\"\n",
        "    return np.random.choice([0, 1], size=dim)\n",
        "\n",
        "def encode_to_schema(schema, data):\n",
        "    \"\"\"Encode data into a structured hypervector based on the schema.\"\"\"\n",
        "    hypervector = np.zeros(DIMENSIONS)\n",
        "\n",
        "    for category, values in data.items():\n",
        "        if category in schema:\n",
        "            dims = schema[category]\n",
        "            # Fill the hypervector slice with the given data\n",
        "            hypervector[dims] = values\n",
        "\n",
        "    return hypervector\n",
        "\n",
        "def bundle_hypervectors(vectors):\n",
        "    \"\"\"Bundle multiple hypervectors by element-wise summation and normalization.\"\"\"\n",
        "    bundled = np.sum(vectors, axis=0)\n",
        "    return np.sign(bundled)  # Normalize to -1, 0, or +1\n",
        "\n",
        "# Step 3: Define Example Data\n",
        "data_1 = {\n",
        "    \"Attributes\": generate_random_hypervector(10),  # Red, Large\n",
        "    \"Actions\": generate_random_hypervector(10),     # Bounce\n",
        "    \"Context\": generate_random_hypervector(10),     # Air\n",
        "    \"Entity Type\": generate_random_hypervector(10)  # Ball\n",
        "}\n",
        "\n",
        "data_2 = {\n",
        "    \"Attributes\": generate_random_hypervector(10),  # Blue, Small\n",
        "    \"Actions\": generate_random_hypervector(10),     # Roll\n",
        "    \"Context\": generate_random_hypervector(10),     # Ground\n",
        "    \"Entity Type\": generate_random_hypervector(10)  # Ball\n",
        "}\n",
        "\n",
        "# Step 4: Encode Data\n",
        "hypervector_1 = encode_to_schema(SCHEMA, data_1)\n",
        "hypervector_2 = encode_to_schema(SCHEMA, data_2)\n",
        "\n",
        "# Step 5: Bundle Data\n",
        "composite_hypervector = bundle_hypervectors([hypervector_1, hypervector_2])\n",
        "\n",
        "# Step 6: Output Results\n",
        "{\n",
        "    \"Hypervector 1\": hypervector_1.tolist(),\n",
        "    \"Hypervector 2\": hypervector_2.tolist(),\n",
        "    \"Composite Hypervector\": composite_hypervector.tolist()\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcF03q99up0b",
        "outputId": "647ee871-1233-4256-d0c6-97729c1a2022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Hypervector 1': [1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " 'Hypervector 2': [0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " 'Composite Hypervector': [1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the Fashion MNIST dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load Fashion MNIST and preprocess\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train, y_train = x_train[:6000], y_train[:6000]  # Use 10% of the training dataset\n",
        "x_test, y_test = x_test[:1000], y_test[:1000]  # Use a small test subset\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_test = (x_test / 255.0).astype(np.float32)\n",
        "\n",
        "# Re-encode the dataset into hypervectors\n",
        "dim = 1000  # Dimensionality of hypervectors\n",
        "\n",
        "def hdc_encode_image(image, dim=1000):\n",
        "    \"\"\"Convert an image into a high-dimensional hypervector.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    # Generate random hypervectors for each pixel intensity value\n",
        "    pixel_hypervectors = np.random.choice([-1, 1], size=(256, dim))  # Assume pixel values 0–255\n",
        "    # Map each pixel to its corresponding hypervector and sum them\n",
        "    encoded_vector = np.sum([pixel_hypervectors[int(pixel)] for pixel in image.flatten()], axis=0)\n",
        "    return np.sign(encoded_vector)  # Normalize to binary -1/1\n",
        "\n",
        "# Encode training and test datasets\n",
        "x_train_encoded = np.array([hdc_encode_image(img, dim) for img in x_train])\n",
        "x_test_encoded = np.array([hdc_encode_image(img, dim) for img in x_test])\n",
        "\n",
        "# Normalize the encoded hypervectors\n",
        "x_train_encoded_norm = normalize(x_train_encoded, axis=1)\n",
        "x_test_encoded_norm = normalize(x_test_encoded, axis=1)\n",
        "\n",
        "# Classify using the pure HDC classifier\n",
        "hdc_predictions = hdc_classify(x_train_encoded_norm, y_train, x_test_encoded_norm)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "hdc_accuracy = accuracy_score(y_test, hdc_predictions)\n",
        "\n",
        "# Output the HDC test accuracy\n",
        "{\n",
        "    \"HDC Test Accuracy\": hdc_accuracy\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d_ch2aP2hFJ",
        "outputId": "811d4b17-325c-421c-eff6-8ab0cae9f8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HDC Test Accuracy': 0.095}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Improved HDC Encoding Function\n",
        "def hdc_encode_image_v2(image, dim=1000, block_size=4):\n",
        "    \"\"\"Encode an image into a high-dimensional hypervector using pixel blocks.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    rows, cols = image.shape\n",
        "    pixel_hypervectors = np.random.choice([-1, 1], size=(256, dim))  # 256 unique intensity levels\n",
        "    encoded_vector = np.zeros(dim)\n",
        "\n",
        "    # Iterate over image blocks\n",
        "    for i in range(0, rows, block_size):\n",
        "        for j in range(0, cols, block_size):\n",
        "            block = image[i:i+block_size, j:j+block_size]\n",
        "            avg_intensity = int(block.mean() * 255)  # Average pixel intensity\n",
        "            encoded_vector += pixel_hypervectors[avg_intensity]\n",
        "\n",
        "    return np.sign(encoded_vector)  # Normalize to binary -1/1\n",
        "\n",
        "# Debugging HDC Pipeline Function\n",
        "def debug_hdc_pipeline(train_hv, train_labels, test_hv, test_labels):\n",
        "    \"\"\"Simplified debugging function for HDC classification.\"\"\"\n",
        "    predictions = []\n",
        "    for test_vector in test_hv:\n",
        "        similarities = cosine_similarity([test_vector], train_hv)[0]\n",
        "        predictions.append(train_labels[np.argmax(similarities)])\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    print(f\"HDC Test Accuracy: {accuracy}\")\n",
        "    return predictions\n",
        "\n",
        "# Main Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "    # Load Fashion MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "    # Use 10% of the dataset for efficiency\n",
        "    x_train, y_train = x_train[:6000], y_train[:6000]\n",
        "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
        "\n",
        "    # Normalize pixel values\n",
        "    x_train = (x_train / 255.0).astype(np.float32)\n",
        "    x_test = (x_test / 255.0).astype(np.float32)\n",
        "\n",
        "    # Re-encode the dataset into hypervectors\n",
        "    dim = 1000  # Dimensionality of hypervectors\n",
        "\n",
        "    print(\"Encoding training data...\")\n",
        "    x_train_encoded = np.array([hdc_encode_image_v2(img, dim) for img in x_train])\n",
        "\n",
        "    print(\"Encoding test data...\")\n",
        "    x_test_encoded = np.array([hdc_encode_image_v2(img, dim) for img in x_test])\n",
        "\n",
        "    # Normalize the encoded hypervectors\n",
        "    x_train_encoded_norm = normalize(x_train_encoded, axis=1)\n",
        "    x_test_encoded_norm = normalize(x_test_encoded, axis=1)\n",
        "\n",
        "    # Run the HDC classifier\n",
        "    print(\"Running HDC classification...\")\n",
        "    debug_hdc_pipeline(x_train_encoded_norm, y_train, x_test_encoded_norm, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UusoUtNk4_d2",
        "outputId": "b6b82b81-ed17-4af8-8997-f358059a2755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding training data...\n",
            "Encoding test data...\n",
            "Running HDC classification...\n",
            "HDC Test Accuracy: 0.095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the dimensions for hypervectors\n",
        "dim = 10000  # High dimensional space\n",
        "\n",
        "# Step 1: Generate random hypervectors for words\n",
        "def generate_random_hypervector(dim):\n",
        "    \"\"\"Generate a random bipolar hypervector.\"\"\"\n",
        "    return np.random.choice([-1, 1], size=dim)\n",
        "\n",
        "# Create a vocabulary with random hypervectors for each unique word\n",
        "def build_vocab(paragraph, dim):\n",
        "    \"\"\"Build a vocabulary of words mapped to random hypervectors.\"\"\"\n",
        "    words = set(paragraph.lower().split())\n",
        "    vocab = {word: generate_random_hypervector(dim) for word in words}\n",
        "    return vocab\n",
        "\n",
        "# Step 2: Encode the paragraph into a single hypervector\n",
        "def encode_paragraph(paragraph, vocab, dim):\n",
        "    \"\"\"Encode the paragraph into a high-dimensional hypervector by bundling word vectors.\"\"\"\n",
        "    words = paragraph.lower().split()\n",
        "    encoded_vector = np.zeros(dim)\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            encoded_vector += vocab[word]\n",
        "    return np.sign(encoded_vector)  # Normalize to binary -1/1\n",
        "\n",
        "# Step 3: Decode the hypervector back to the most similar words\n",
        "def decode_hypervector(encoded_vector, vocab, top_n=5):\n",
        "    \"\"\"Find the most similar words to the encoded vector using cosine similarity.\"\"\"\n",
        "    similarities = {word: cosine_similarity([encoded_vector], [hv])[0][0] for word, hv in vocab.items()}\n",
        "    # Sort by similarity and return the top N words\n",
        "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# Example paragraph\n",
        "paragraph = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab(paragraph, dim)\n",
        "\n",
        "# Encode the paragraph\n",
        "encoded_vector = encode_paragraph(paragraph, vocab, dim)\n",
        "\n",
        "# Decode the hypervector\n",
        "decoded_words = decode_hypervector(encoded_vector, vocab)\n",
        "\n",
        "# Output the results\n",
        "print(\"Encoded Hypervector:\", encoded_vector)\n",
        "print(\"Decoded Words:\", decoded_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX5Wp4zC57nk",
        "outputId": "dba0f603-51b8-4cba-ee41-8ad7288abc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Hypervector: [-1. -1. -1. ... -1.  1. -1.]\n",
            "Decoded Words: [('the', 0.535199999999999), ('brown', 0.2513999999999994), ('dog', 0.25079999999999936), ('lazy', 0.2443999999999994), ('over', 0.23999999999999944)]\n"
          ]
        }
      ]
    }
  ]
}